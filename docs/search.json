[
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "Day 2",
    "section": "",
    "text": "Yesterday we covered:\n\nHow to use Unix.\nHow to get onto HAWK.\nHow to fetch the Github repository and download the resources.\nHow to execute the scrnaseq nextflow pipeline on Hawk.\n\n\n\n\n\n\nThe output from the nextflow execution run is stored under results/cellranger_count_1k_mouse_kidney_CNIK_3pv3\nLogin to your Hawk account and access the folder:\n\ncd /scratch/c.123456/scrnaseq-course/scrnaseq/results\n\ncd cellranger_count_1k_mouse_kidney_CNIK_3pv3\n\n\n\n\nYou can skip this step if you are working on RStudio Server.\nOnce the counts are generated using nextflow pipeline, download the data onto your personal computer.\nFor this, you will need to use Filezilla to copy the files from remote onto your local machine.\nFollow the steps below:\n\n\nType in the Host: hawklogin.cf.ac.uk\nUsername: \nPassword: \nPort: 22\n\n\nClick on Quickconnect\nNavigate to the folder scrnaseq-course/scrnaseq/results\nCreate a folder on your local machine, called scrnaseq-nextflow.\nCreate 4 folders within scrnaseq-nextflow folder:\n\nbin\ninput\noutput\nresources\n\nNow, Drag and Drop the files from the results folder in the remote server to your input folder within scrnaseq-nextflow folder on your local machine.\nOnce the files are copied, these should be visible on your local machine under input folder.\n\n\n\n\n\n\nOnce the counts have been downloaded into the input folder on your local machine, Start RStudio on Mac/Windows.\n\n\n\nClick on New File –&gt; R Markdown...\n\n\n\nThis is will open an Untiltled file on the top-left hand side.\n\n\n\nDelete lines from 7 until 29 except first 5-6 lines.\n\n\n\n\n\nCopy the line in your markdown document:\n\n\n\nCode\n# load libraries\nlibrary(Seurat)\n\n\nLoading required package: SeuratObject\n\n\nLoading required package: sp\n\n\n'SeuratObject' was built under R 4.5.0 but the current version is\n4.5.2; it is recomended that you reinstall 'SeuratObject' as the ABI\nfor R may have changed\n\n\n\nAttaching package: 'SeuratObject'\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, t\n\n\n\nPress the green arrow key on line 7 which says Run Current Chunk to load the Seurat library.\n\n\n\n\n\n\nWe start by reading in the data. The Read10X() function reads in the output of the cellranger pipeline from 10X, returning a unique molecular identified (UMI) count matrix.\nThe values in this matrix represent the number of molecules for each feature (i.e. gene; row) that are detected in each cell (column).\n\n# Load the 1k mouse kidney dataset\ndat2 &lt;- Read10X(data.dir = \"input/filtered_feature_bc_matrix/\")\n\nWe next use the count matrix to create a Seurat object.\n\n# Initialize the Seurat object with the raw (non-normalized data).\ndat2Obj &lt;- CreateSeuratObject(counts = dat2, project = \"1KmouseKidney\", min.cells = 3, min.features = 200)\n\ndat2Obj\n\n\n\n\nSeurat allows you to easily explore QC metrics and filter cells based on any user-defined criteria.\nThese includes:\n\nThe number of unique genes detected in each cell.\n\nLow-quality cells or empty droplets will often have very few genes\nCell doublets or multiplets may exhibit an aberrantly high gene count\n\nThe percentage of reads that map to the mitochondrial genome.\n\nLow-quality / dying cells often exhibit extensive mitochondrial contamination\nWe calculate mitochondrial QC metrics with the PercentageFeatureSet() function\nWe use the set of all genes starting with MT- or mt- as a set of mitochondrial genes\n\n\n\n# The [[ operator can add columns to object metadata.\ndat2Obj[[\"percent.mt\"]] &lt;- PercentageFeatureSet(dat2Obj, pattern = \"^mt-\")\n\nTo visualize QC metrics, we use VlnPlot() function.\n\n# Visualize QC metrics as a violin plot\nVlnPlot(dat2Obj, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\n\nTry using ^MT- as pattern. What do you get?\ndat2Obj[[\"percent.mt\"]] &lt;- PercentageFeatureSet(dat2Obj, pattern = \"^MT-\")\n\nNext, you can plot a FeatureScatter plot for visualising feature-feature relationships.\n\n# Plot FeatureScatter plot\nplot1 &lt;- FeatureScatter(dat2Obj, feature1 = \"nCount_RNA\", feature2 = \"percent.mt\")\nplot2 &lt;- FeatureScatter(dat2Obj, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\")\nplot1 + plot2\n\n\nNow, we will filter cells that have unique feature counts over 9000 or less than 200\nWe filter cells that have &gt;5% mitochondrial counts\n\ndat2FilteredObj &lt;- subset(dat2Obj, subset = nFeature_RNA &gt; 200 & nFeature_RNA &lt; 9000 &  percent.mt &lt; 5)\n\ndat2FilteredObj\n# An object of class Seurat \n# 18321 features across 1235 samples within 1 assay \n# Active assay: RNA (18321 features, 0 variable features)\n#  1 layer present: counts\nLets break it down: - nFeature_RNA &gt; 200: Keeps cells that have more than 200 detected genes (features). This removes low-quality cells or empty droplets. - Feature_RNA &lt; 9000: Removes cells with too many detected genes, which might indicate doublets (two cells captured in one droplet). - percent.mt &lt; 5: Filters out cells where mitochondrial gene percentage is greater than 5%. High mitochondrial content often indicates damaged or dying cells.\n\nWith this command, the number of cells reduces to 1235.\n\n\n\n\n\nAfter removing unwanted cells from the dataset, the next step is to normalize the data.\nBy default, we employ a global-scaling normalization method LogNormalize that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.\nNormalized values are stored in dat2NormalisedObj[[\"RNA\"]]$data\n\ndat2NormalisedObj &lt;- NormalizeData(object = dat2FilteredObj, normalization.method = \"LogNormalize\", scale.factor = 10000)\n\nThe use of SCTransform replaces the need to run NormalizeData, FindVariableFeatures, or ScaleData.\n\n\n\n\n\nWe next calculate a subset of features (genes) that exhibit high cell-to-cell variation in the dataset.\nThese are the features those are highly expressed in some cells, and lowly expressed in others.\nIt has been found Brennecke et al. Nat Methods.2013 that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.\n\ndat2Normalised2Obj &lt;- FindVariableFeatures(dat2NormalisedObj, selection.method = \"vst\", nfeatures = 2000)\ntop10 &lt;- head(VariableFeatures(dat2Normalised2Obj), 10)\nplot3 &lt;- VariableFeaturePlot(dat2Normalised2Obj)\nplot4 &lt;- LabelPoints(plot = plot3, points = top10, repel = TRUE)\nCombinePlots(plots = list(plot3, plot4))\n\n\n\n\n\nThe ScaleData function in Seurat is used to normalize and scale gene expression values across cells.\nAdditionally, it can regress out unwanted sources of variation, such as sequencing depth (nUMI) or mitochondrial gene percentage (percent.mito), helping to remove technical artifacts.\nBy default, only variable features are scaled.\nBut if you observe, in out script, we have used rownames(dat2Normalised2Obj) which applies scaling to all genes which is stored in all.genes object.\nThis is then applied to ScaleData function which uses all.genes as features for scaling.\n\nall.genes &lt;- rownames(dat2Normalised2Obj)\ndat2ScaledObj &lt;- ScaleData(object = dat2Normalised2Obj, features = all.genes)\n\n\n\nTo remove unwanted sources of variation from a single-cell dataset, such as “percent.mt”, use “vars.to.regress” to include these:\n\ndat2ScaledObj &lt;- ScaleData(object = dat2Normalised2Obj, features = all.genes, vars.to.regress = \"percent.mt\")\n\n\n\n\n\nNext we perform PCA on the scaled data.\nBy default, only the previously determined variable features are used as input, but can be defined using features argument if you wish to choose a different subset.\n\ndat2Scaled2Obj &lt;- RunPCA(object = dat2ScaledObj, features = VariableFeatures(object = dat2ScaledObj), do.print = TRUE, ndims.print = 1:5, nfeatures.print = 5)\n\nThis command uses only the highly variable genes (VariableFeatures(object = dat2ScaledObj)) instead of all genes to reduce noise and improve clustering.\ndo.print = TRUE prints the top genes contributing to each principal component (PC).\npcs.print = 1:5 displays the results for PC1 to PC5 (i.e., the first five principal components).\nPC1 (Principal Component 1) is the most important axes of variation.\nPC2, PC3, PC4, etc. explain additional variation in the data.\nTop Positive Genes (e.g., Ldb2, Ebf1, Prkg1, Meis2) are more highly expressed in one subset of cells.\nTop Negative Genes (e.g., Slc27a2, Keg1, Sugct) are more highly expressed in another subset.\nPC1 may capture a major biological signal, such as cell type differences.\nIf PC2 separates a different cell population, it might represent a different biological process, like a cell cycle state or differentiation.\nDimHeatmap() allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses.\n\nDimHeatmap(\n    object = dat2Scaled2Obj, \n    dims = 1, \n    balanced = TRUE\n)\n\n\nNow, use dims 1 to 15 to plot 15 PCs to observe which PCs contribute to sources of variation in the dataset.\n\nDimHeatmap(\n    object = dat2Scaled2Obj, \n    dims = 1:15, \n    balanced = TRUE\n)\n\n\n\n\n\nAfter running PCA using RunPCA(), you need to determine how many PCs to retain for clustering and downstream analysis.\nOne common method is using the Elbow Plot.\nThe top principal components represents highest variations in the dataset.\nSo, how many components should we choose to include? 10? 20? 100?\nIn ‘Elbow plot’ it ranks principle components based on the percentage of variance explained by each one.\n\nElbowPlot(dat2Scaled2Obj)\n\n\nIf we are considering going with the default settings with ndims = 20, then we can clearly see that at the elbow point is at PC 11 which accounts for most variation in the dataset.\nHowever, if we choose to include ndims = 50, then the elbow plot changes and includes 50 PCs.\n\nElbowPlot(dat2Scaled2Obj, ndims = 50)\n\n\nNow, this gives us a much broader picture and also shows more PCs are contributing to variation beyond PC 11.\nWith this plot, we can keep the value between PC 21-25.\nIn this tutorial, I have chosen PC 25 to stretch the limit and include smaller variance. But the value of 21 will also work fine.\n\n\n\n\n\nSeurat applies a graph-based clustering approach, called Shared Nearest Neighbors (SNN).\nHow the SNN Score is Computed:\n\nIf cell A’s neighbor list and cell B’s neighbor list share many cells, they are likely to be in the same cluster.\nThe SNN score (edge weight) is higher if two cells have more shared neighbors.\n\nExample:\n\nCell A’s neighbors: {B, C, D, E}\nCell B’s neighbors: {A, C, D, F}\nCell A and Cell B share: {C, D}\nSNN score = Number of shared neighbors / Minimum kNN list size\nSNN score = 2 / 4 = 0.5\n\nIf two cells share many of their nearest neighbors, their SNN score is high, and they are more likely to belong to the same cluster.\n\n\n\nSo, the first step is performed using the FindNeighbors() function, and takes as input the previously defined dimensionality of the dataset (first 25 PCs).\nTo cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM to iteratively group cells together.\nThe FindClusters() function implements this procedure, and contains a resolution parameter that sets the ‘granularity’ of the downstream clustering, with increased values leading to a greater number of clusters.\n\nRun the following commands:\nseuratFindNeighbors &lt;- FindNeighbors(\n    dat2Scaled2Obj,\n    reduction = \"pca\",\n    dims = 1:25,\n    do.plot = FALSE,\n    graph.name = NULL,\n    k.param = 30\n)\n\nseuratFindClusters &lt;- FindClusters(object = seuratFindNeighbors)"
  },
  {
    "objectID": "day2.html#recap-from-day-1",
    "href": "day2.html#recap-from-day-1",
    "title": "Day 2",
    "section": "",
    "text": "Yesterday we covered:\n\nHow to use Unix.\nHow to get onto HAWK.\nHow to fetch the Github repository and download the resources.\nHow to execute the scrnaseq nextflow pipeline on Hawk."
  },
  {
    "objectID": "day2.html#outputs-from-the-nextflow-pipeline",
    "href": "day2.html#outputs-from-the-nextflow-pipeline",
    "title": "Day 2",
    "section": "",
    "text": "The output from the nextflow execution run is stored under results/cellranger_count_1k_mouse_kidney_CNIK_3pv3\nLogin to your Hawk account and access the folder:\n\ncd /scratch/c.123456/scrnaseq-course/scrnaseq/results\n\ncd cellranger_count_1k_mouse_kidney_CNIK_3pv3"
  },
  {
    "objectID": "day2.html#downloading-data-for-analysis-in-seurat-for-analysis-on-rstudio-on-local-system",
    "href": "day2.html#downloading-data-for-analysis-in-seurat-for-analysis-on-rstudio-on-local-system",
    "title": "Day 2",
    "section": "",
    "text": "You can skip this step if you are working on RStudio Server.\nOnce the counts are generated using nextflow pipeline, download the data onto your personal computer.\nFor this, you will need to use Filezilla to copy the files from remote onto your local machine.\nFollow the steps below:\n\n\nType in the Host: hawklogin.cf.ac.uk\nUsername: \nPassword: \nPort: 22\n\n\nClick on Quickconnect\nNavigate to the folder scrnaseq-course/scrnaseq/results\nCreate a folder on your local machine, called scrnaseq-nextflow.\nCreate 4 folders within scrnaseq-nextflow folder:\n\nbin\ninput\noutput\nresources\n\nNow, Drag and Drop the files from the results folder in the remote server to your input folder within scrnaseq-nextflow folder on your local machine.\nOnce the files are copied, these should be visible on your local machine under input folder."
  },
  {
    "objectID": "day2.html#processing-scrnaseq-data-1k_mouse_kidney_cnik_3pv3-in-seurat",
    "href": "day2.html#processing-scrnaseq-data-1k_mouse_kidney_cnik_3pv3-in-seurat",
    "title": "Day 2",
    "section": "",
    "text": "Once the counts have been downloaded into the input folder on your local machine, Start RStudio on Mac/Windows.\n\n\n\nClick on New File –&gt; R Markdown...\n\n\n\nThis is will open an Untiltled file on the top-left hand side.\n\n\n\nDelete lines from 7 until 29 except first 5-6 lines.\n\n\n\n\n\nCopy the line in your markdown document:\n\n\n\nCode\n# load libraries\nlibrary(Seurat)\n\n\nLoading required package: SeuratObject\n\n\nLoading required package: sp\n\n\n'SeuratObject' was built under R 4.5.0 but the current version is\n4.5.2; it is recomended that you reinstall 'SeuratObject' as the ABI\nfor R may have changed\n\n\n\nAttaching package: 'SeuratObject'\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, t\n\n\n\nPress the green arrow key on line 7 which says Run Current Chunk to load the Seurat library.\n\n\n\n\n\n\nWe start by reading in the data. The Read10X() function reads in the output of the cellranger pipeline from 10X, returning a unique molecular identified (UMI) count matrix.\nThe values in this matrix represent the number of molecules for each feature (i.e. gene; row) that are detected in each cell (column).\n\n# Load the 1k mouse kidney dataset\ndat2 &lt;- Read10X(data.dir = \"input/filtered_feature_bc_matrix/\")\n\nWe next use the count matrix to create a Seurat object.\n\n# Initialize the Seurat object with the raw (non-normalized data).\ndat2Obj &lt;- CreateSeuratObject(counts = dat2, project = \"1KmouseKidney\", min.cells = 3, min.features = 200)\n\ndat2Obj\n\n\n\n\nSeurat allows you to easily explore QC metrics and filter cells based on any user-defined criteria.\nThese includes:\n\nThe number of unique genes detected in each cell.\n\nLow-quality cells or empty droplets will often have very few genes\nCell doublets or multiplets may exhibit an aberrantly high gene count\n\nThe percentage of reads that map to the mitochondrial genome.\n\nLow-quality / dying cells often exhibit extensive mitochondrial contamination\nWe calculate mitochondrial QC metrics with the PercentageFeatureSet() function\nWe use the set of all genes starting with MT- or mt- as a set of mitochondrial genes\n\n\n\n# The [[ operator can add columns to object metadata.\ndat2Obj[[\"percent.mt\"]] &lt;- PercentageFeatureSet(dat2Obj, pattern = \"^mt-\")\n\nTo visualize QC metrics, we use VlnPlot() function.\n\n# Visualize QC metrics as a violin plot\nVlnPlot(dat2Obj, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\n\nTry using ^MT- as pattern. What do you get?\ndat2Obj[[\"percent.mt\"]] &lt;- PercentageFeatureSet(dat2Obj, pattern = \"^MT-\")\n\nNext, you can plot a FeatureScatter plot for visualising feature-feature relationships.\n\n# Plot FeatureScatter plot\nplot1 &lt;- FeatureScatter(dat2Obj, feature1 = \"nCount_RNA\", feature2 = \"percent.mt\")\nplot2 &lt;- FeatureScatter(dat2Obj, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\")\nplot1 + plot2\n\n\nNow, we will filter cells that have unique feature counts over 9000 or less than 200\nWe filter cells that have &gt;5% mitochondrial counts\n\ndat2FilteredObj &lt;- subset(dat2Obj, subset = nFeature_RNA &gt; 200 & nFeature_RNA &lt; 9000 &  percent.mt &lt; 5)\n\ndat2FilteredObj\n# An object of class Seurat \n# 18321 features across 1235 samples within 1 assay \n# Active assay: RNA (18321 features, 0 variable features)\n#  1 layer present: counts\nLets break it down: - nFeature_RNA &gt; 200: Keeps cells that have more than 200 detected genes (features). This removes low-quality cells or empty droplets. - Feature_RNA &lt; 9000: Removes cells with too many detected genes, which might indicate doublets (two cells captured in one droplet). - percent.mt &lt; 5: Filters out cells where mitochondrial gene percentage is greater than 5%. High mitochondrial content often indicates damaged or dying cells.\n\nWith this command, the number of cells reduces to 1235.\n\n\n\n\n\nAfter removing unwanted cells from the dataset, the next step is to normalize the data.\nBy default, we employ a global-scaling normalization method LogNormalize that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.\nNormalized values are stored in dat2NormalisedObj[[\"RNA\"]]$data\n\ndat2NormalisedObj &lt;- NormalizeData(object = dat2FilteredObj, normalization.method = \"LogNormalize\", scale.factor = 10000)\n\nThe use of SCTransform replaces the need to run NormalizeData, FindVariableFeatures, or ScaleData.\n\n\n\n\n\nWe next calculate a subset of features (genes) that exhibit high cell-to-cell variation in the dataset.\nThese are the features those are highly expressed in some cells, and lowly expressed in others.\nIt has been found Brennecke et al. Nat Methods.2013 that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.\n\ndat2Normalised2Obj &lt;- FindVariableFeatures(dat2NormalisedObj, selection.method = \"vst\", nfeatures = 2000)\ntop10 &lt;- head(VariableFeatures(dat2Normalised2Obj), 10)\nplot3 &lt;- VariableFeaturePlot(dat2Normalised2Obj)\nplot4 &lt;- LabelPoints(plot = plot3, points = top10, repel = TRUE)\nCombinePlots(plots = list(plot3, plot4))\n\n\n\n\n\nThe ScaleData function in Seurat is used to normalize and scale gene expression values across cells.\nAdditionally, it can regress out unwanted sources of variation, such as sequencing depth (nUMI) or mitochondrial gene percentage (percent.mito), helping to remove technical artifacts.\nBy default, only variable features are scaled.\nBut if you observe, in out script, we have used rownames(dat2Normalised2Obj) which applies scaling to all genes which is stored in all.genes object.\nThis is then applied to ScaleData function which uses all.genes as features for scaling.\n\nall.genes &lt;- rownames(dat2Normalised2Obj)\ndat2ScaledObj &lt;- ScaleData(object = dat2Normalised2Obj, features = all.genes)\n\n\n\nTo remove unwanted sources of variation from a single-cell dataset, such as “percent.mt”, use “vars.to.regress” to include these:\n\ndat2ScaledObj &lt;- ScaleData(object = dat2Normalised2Obj, features = all.genes, vars.to.regress = \"percent.mt\")\n\n\n\n\n\nNext we perform PCA on the scaled data.\nBy default, only the previously determined variable features are used as input, but can be defined using features argument if you wish to choose a different subset.\n\ndat2Scaled2Obj &lt;- RunPCA(object = dat2ScaledObj, features = VariableFeatures(object = dat2ScaledObj), do.print = TRUE, ndims.print = 1:5, nfeatures.print = 5)\n\nThis command uses only the highly variable genes (VariableFeatures(object = dat2ScaledObj)) instead of all genes to reduce noise and improve clustering.\ndo.print = TRUE prints the top genes contributing to each principal component (PC).\npcs.print = 1:5 displays the results for PC1 to PC5 (i.e., the first five principal components).\nPC1 (Principal Component 1) is the most important axes of variation.\nPC2, PC3, PC4, etc. explain additional variation in the data.\nTop Positive Genes (e.g., Ldb2, Ebf1, Prkg1, Meis2) are more highly expressed in one subset of cells.\nTop Negative Genes (e.g., Slc27a2, Keg1, Sugct) are more highly expressed in another subset.\nPC1 may capture a major biological signal, such as cell type differences.\nIf PC2 separates a different cell population, it might represent a different biological process, like a cell cycle state or differentiation.\nDimHeatmap() allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses.\n\nDimHeatmap(\n    object = dat2Scaled2Obj, \n    dims = 1, \n    balanced = TRUE\n)\n\n\nNow, use dims 1 to 15 to plot 15 PCs to observe which PCs contribute to sources of variation in the dataset.\n\nDimHeatmap(\n    object = dat2Scaled2Obj, \n    dims = 1:15, \n    balanced = TRUE\n)\n\n\n\n\n\nAfter running PCA using RunPCA(), you need to determine how many PCs to retain for clustering and downstream analysis.\nOne common method is using the Elbow Plot.\nThe top principal components represents highest variations in the dataset.\nSo, how many components should we choose to include? 10? 20? 100?\nIn ‘Elbow plot’ it ranks principle components based on the percentage of variance explained by each one.\n\nElbowPlot(dat2Scaled2Obj)\n\n\nIf we are considering going with the default settings with ndims = 20, then we can clearly see that at the elbow point is at PC 11 which accounts for most variation in the dataset.\nHowever, if we choose to include ndims = 50, then the elbow plot changes and includes 50 PCs.\n\nElbowPlot(dat2Scaled2Obj, ndims = 50)\n\n\nNow, this gives us a much broader picture and also shows more PCs are contributing to variation beyond PC 11.\nWith this plot, we can keep the value between PC 21-25.\nIn this tutorial, I have chosen PC 25 to stretch the limit and include smaller variance. But the value of 21 will also work fine.\n\n\n\n\n\nSeurat applies a graph-based clustering approach, called Shared Nearest Neighbors (SNN).\nHow the SNN Score is Computed:\n\nIf cell A’s neighbor list and cell B’s neighbor list share many cells, they are likely to be in the same cluster.\nThe SNN score (edge weight) is higher if two cells have more shared neighbors.\n\nExample:\n\nCell A’s neighbors: {B, C, D, E}\nCell B’s neighbors: {A, C, D, F}\nCell A and Cell B share: {C, D}\nSNN score = Number of shared neighbors / Minimum kNN list size\nSNN score = 2 / 4 = 0.5\n\nIf two cells share many of their nearest neighbors, their SNN score is high, and they are more likely to belong to the same cluster.\n\n\n\nSo, the first step is performed using the FindNeighbors() function, and takes as input the previously defined dimensionality of the dataset (first 25 PCs).\nTo cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM to iteratively group cells together.\nThe FindClusters() function implements this procedure, and contains a resolution parameter that sets the ‘granularity’ of the downstream clustering, with increased values leading to a greater number of clusters.\n\nRun the following commands:\nseuratFindNeighbors &lt;- FindNeighbors(\n    dat2Scaled2Obj,\n    reduction = \"pca\",\n    dims = 1:25,\n    do.plot = FALSE,\n    graph.name = NULL,\n    k.param = 30\n)\n\nseuratFindClusters &lt;- FindClusters(object = seuratFindNeighbors)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "scrnaseqcourse",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#what-you-will-learn-on-this-course",
    "href": "index.html#what-you-will-learn-on-this-course",
    "title": "scrnaseqcourse",
    "section": "What you will learn on this course:",
    "text": "What you will learn on this course:\n\nBasic Unix commands\nWorking on HPC\nDownloading scrnaseq datasets and genome files.\nSetting up directory for the data processing and analysis.\nHow to run Nextflow pipeline on HPCs.\nHow to process 10x cellranger pipeline.\nHow to process and analyse scrnaseq count data in R.\nHow to visualise the analysed data and perform additional downstream analysis."
  },
  {
    "objectID": "index.html#introduction-to-single-cell-rna-seq",
    "href": "index.html#introduction-to-single-cell-rna-seq",
    "title": "scrnaseqcourse",
    "section": "Introduction to Single-Cell RNA-seq",
    "text": "Introduction to Single-Cell RNA-seq\n\nRNA-seq allows profiling the transcripts in a sample in an efficient and cost-effective way.\nPart of its success is due to the fact that RNA-seq allows for an unbiased sampling of all transcripts in a sample, rather than being limited to a pre-determined set of transcripts (as in microarrays or RT-qPCR).\nTypically, RNA-seq has been used in samples composed of a mixture of cells, referred to as bulk RNA-seq, and has many applications.\nFor example, it can be used to characterise expression signatures between tissues in healthy/diseased, wild-type/mutant or control/treated samples.\nHowever, with bulk RNA-seq we can only estimate the average expression level for each gene across a population of cells, without regard for the heterogeneity in gene expression across individual cells of that sample.\n\n\n\nUnlike with the bulk approach, with scRNA-seq we can estimate a distribution of expression levels for each gene across a population of cells.\nThis allows us to answer new biological questions where cell-specific changes in the transcriptome are important. For example discovering new or rare cell types, identifying differential cell composition between healthy/diseased tissues or understanding cell differentiation during development."
  },
  {
    "objectID": "index.html#sample-preparation-protocols",
    "href": "index.html#sample-preparation-protocols",
    "title": "scrnaseqcourse",
    "section": "Sample Preparation Protocols",
    "text": "Sample Preparation Protocols\nBroadly speaking, a typical scRNA-seq protocol consists of the following steps (illustrated in the figure below):\n\nTissue dissection and cell dissociating to obtain a suspension of cells.\nOptionally cells may be selected\nCapture single cells into individual reaction containers (e.g. wells or oil droplets).\nExtracting the RNA from each cell.\nReverse-transcribing the RNA to more stable cDNA.\nAmplifying the cDNA (either by in vitro transcription or by PCR).\nPreparing the sequencing library with adequate molecular adapters.\nSequencing, usually with paired-end Illumina protocols.\nProcessing the raw data to obtain a count matrix of genes-by-cells\nCarrying several downstream analysis (the focus of this course)."
  },
  {
    "objectID": "index.html#unix-configuration",
    "href": "index.html#unix-configuration",
    "title": "scrnaseqcourse",
    "section": "Unix: Configuration",
    "text": "Unix: Configuration\n\nUnix: Hardware\n\nHardware: the physical components (the bits you can see when you take the back off a computer.\nCPU (central processing unit): the computer chip performing the calculations.\n\nHard drive: saved storage – your saved files.\nRAM (random access memory): like the hard drive, it stores information. Access (reading this information) is faster than the hard drive, although information is only present whilst the compute is powered on, i.e. this is memory space.\n\n\n\n\nUnix: Kernel\n\nThe kernel sits between the command line and the hardware.\n\nWhen you type a command line to the screen and hit “enter,” the kernel executes the command, allocating the appropriate time and memory to the task.\n\n\n\nUnix: Commands\n\nthe commands are the bread-and-butter functions common to all Linux installations and are part of the core (minimal) install.\nthey perform core processes such as file and job manipulations (read, write, delete files/jobs).\n\n\n\nUnix: Programs\n\nOther programs work identically to the core commands.\nThey are different because they generally perform specialised functions, for example, mapping next-generation sequence to a genome assembly."
  },
  {
    "objectID": "index.html#basic-unix-graphical-user-interface-gui-command-lineshell",
    "href": "index.html#basic-unix-graphical-user-interface-gui-command-lineshell",
    "title": "scrnaseqcourse",
    "section": "Basic Unix: Graphical User Interface (GUI) & Command Line/Shell",
    "text": "Basic Unix: Graphical User Interface (GUI) & Command Line/Shell\n\nAll Windows/Mac/Linux PC’s use a GUI to allow users to easily navigate and use the PC.\nThe GUI is common to Window/Mac/Linux.\n\nThe interface permits you to access files and run commands/programs by clicking icon/apps, drag-and-drop.\n\nThis is a shell. We can use this to type commands etc.\n\nIn simple terms, the window on the computer into which you type a command, is called the shell"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]