---
title: "Day 2"
format:
  html:
    code-fold: true
    code-tools: true
    code-block-bg: true
    code-block-border-left: "#31BAE9"
---

# Processing scRNAseq reads generated from Nextflow pipeline

## Recap from Day 1

- Yesterday we covered:
  - How to use Unix.
  - How to get onto HAWK.
  - How to fetch the Github repository and download the resources.
  - How to execute the scrnaseq nextflow pipeline on Hawk.
  
## Outputs from the nextflow pipeline

- The output from the nextflow execution run is stored under `results/cellranger_count_1k_mouse_kidney_CNIK_3pv3`
- Login to your Hawk account and access the folder:

```
cd /scratch/c.123456/scrnaseq-course/scrnaseq/results

cd cellranger_count_1k_mouse_kidney_CNIK_3pv3
```

## Downloading data for analysis in Seurat (for analysis on RStudio on local system)

- You can skip this step if you are working on RStudio Server.
- Once the counts are generated using nextflow pipeline, download the data onto your personal computer.
- For this, you will need to use Filezilla to copy the files from remote onto your local machine.
- Follow the steps below:

1. Type in the Host: hawklogin.cf.ac.uk
2. Username: <hawk username>
3. Password: <hawk password>
4. Port: 22

- Click on `Quickconnect`
- Navigate to the folder `scrnaseq-course/scrnaseq/results`
- Create a folder on your local machine, called `scrnaseq-nextflow`.
- Create 4 folders within `scrnaseq-nextflow` folder:
  - bin
  - input
  - output
  - resources
- Now, Drag and Drop the files from the `results` folder in the remote server to your `input` folder within `scrnaseq-nextflow` folder on your local machine.
- Once the files are copied, these should be visible on your local machine under `input` folder.

![](images/filezilla-login.png){width=100%}

## Processing scrnaseq data (1k_mouse_kidney_CNIK_3pv3) in Seurat 

- Once the counts have been downloaded into the `input` folder on your local machine, Start **RStudio** on Mac/Windows.

![](images/rstudio.png){width=100%}

- Click on `New File` --> `R Markdown...`

![](images/rstudio-newfile.png){width=100%}

- This is will open an `Untiltled` file on the top-left hand side.

![](images/rstudio-untilted-rmarkdown.png){width=100%}

- Delete lines from 7 until 29 except first 5-6 lines.

![](images/rmarkdown-2.png){width=100%}

### Load libraries

- Copy the line in your markdown document:

```{r}
#| echo: true
#| code-overflow: wrap
# load libraries
library(Seurat)
```

- Press the green arrow key on line 7 which says **Run Current Chunk** to load the Seurat library.

![](images/rmarkdown-1.png){width=100%}

### Setup the Seurat object

- We start by reading in the data. The `Read10X()` function reads in the output of the cellranger pipeline from 10X, returning a unique molecular identified (UMI) count matrix.
- The values in this matrix represent the number of molecules for each feature (i.e. gene; row) that are detected in each cell (column).

```r
# Load the 1k mouse kidney dataset
dat2 <- Read10X(data.dir = "input/filtered_feature_bc_matrix/")
```

- We next use the count matrix to create a Seurat object. 

```r
# Initialize the Seurat object with the raw (non-normalized data).
dat2Obj <- CreateSeuratObject(counts = dat2, project = "1KmouseKidney", min.cells = 3, min.features = 200)

dat2Obj
```

#### QC and filtering cells for further analysis

- Seurat allows you to easily explore QC metrics and filter cells based on any user-defined criteria.
- These includes:
  - The number of unique genes detected in each cell.
    - Low-quality cells or empty droplets will often have very few genes
    - Cell doublets or multiplets may exhibit an aberrantly high gene count
  - The percentage of reads that map to the mitochondrial genome.
    - Low-quality / dying cells often exhibit extensive mitochondrial contamination
    - We calculate mitochondrial QC metrics with the `PercentageFeatureSet()` function
    - We use the set of all genes starting with `MT-` or `mt-` as a set of mitochondrial genes

```r
# The [[ operator can add columns to object metadata.
dat2Obj[["percent.mt"]] <- PercentageFeatureSet(dat2Obj, pattern = "^mt-")
```

- To visualize QC metrics, we use `VlnPlot()` function.

```r
# Visualize QC metrics as a violin plot
VlnPlot(dat2Obj, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```

![](images/Violin_plot_1k_data.png){width=100%}

Try using ^MT- as pattern. What do you get?

```r
dat2Obj[["percent.mt"]] <- PercentageFeatureSet(dat2Obj, pattern = "^MT-")
```

- Next, you can plot a FeatureScatter plot for visualising feature-feature relationships.

```r
# Plot FeatureScatter plot
plot1 <- FeatureScatter(dat2Obj, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(dat2Obj, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2
```

![](images/FeatureScatter.png){width=100%}

- Now, we will filter cells that have unique feature counts over 9000 or less than 200
- We filter cells that have >5% mitochondrial counts

```r
dat2FilteredObj <- subset(dat2Obj, subset = nFeature_RNA > 200 & nFeature_RNA < 9000 &  percent.mt < 5)

dat2FilteredObj
# An object of class Seurat 
# 18321 features across 1235 samples within 1 assay 
# Active assay: RNA (18321 features, 0 variable features)
#  1 layer present: counts
```

Lets break it down:
- `nFeature_RNA > 200`: Keeps cells that have more than 200 detected genes (features). This removes low-quality cells or empty droplets.
- `Feature_RNA < 9000`: Removes cells with too many detected genes, which might indicate doublets (two cells captured in one droplet).
- `percent.mt < 5`: Filters out cells where mitochondrial gene percentage is greater than 5%. High mitochondrial content often indicates damaged or dying cells.

- With this command, the number of cells reduces to 1235.

### Normalizing the data

- After removing unwanted cells from the dataset, the next step is to normalize the data.
- By default, we employ a global-scaling normalization method `LogNormalize` that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.
- Normalized values are stored in `dat2NormalisedObj[["RNA"]]$data`

```r
dat2NormalisedObj <- NormalizeData(object = dat2FilteredObj, normalization.method = "LogNormalize", scale.factor = 10000)
```

- The use of `SCTransform` replaces the need to run `NormalizeData`, `FindVariableFeatures`, or `ScaleData`.

### Identification of highly variable features (feature selection)

- We next calculate a subset of features (genes) that exhibit high cell-to-cell variation in the dataset.
- These are the features those are highly expressed in some cells, and lowly expressed in others.
- It has been found [Brennecke et al. Nat Methods.2013](https://www.nature.com/articles/nmeth.2645) that focusing on these genes in downstream analysis helps to highlight biological signal in single-cell datasets.

```r
dat2Normalised2Obj <- FindVariableFeatures(dat2NormalisedObj, selection.method = "vst", nfeatures = 2000)
top10 <- head(VariableFeatures(dat2Normalised2Obj), 10)
plot3 <- VariableFeaturePlot(dat2Normalised2Obj)
plot4 <- LabelPoints(plot = plot3, points = top10, repel = TRUE)
CombinePlots(plots = list(plot3, plot4))
```

![](images/VariableGenesPlot.png){width=100%}

### Scaling the data

- The `ScaleData` function in Seurat is used to normalize and scale gene expression values across cells.
- Additionally, it can **regress out unwanted sources of variation**, such as sequencing depth (`nUMI`) or mitochondrial gene percentage (`percent.mito`), helping to remove technical artifacts.
- By default, only variable features are scaled.
- But if you observe, in out script, we have used `rownames(dat2Normalised2Obj)` which applies scaling to all genes which is stored in `all.genes` object.
- This is then applied to `ScaleData` function which uses `all.genes` as features for scaling.

```r
all.genes <- rownames(dat2Normalised2Obj)
dat2ScaledObj <- ScaleData(object = dat2Normalised2Obj, features = all.genes)
```

#### How can I remove unwanted sources of variation ?

- To remove unwanted sources of variation from a single-cell dataset, such as "percent.mt", use "vars.to.regress" to include these:

```r
dat2ScaledObj <- ScaleData(object = dat2Normalised2Obj, features = all.genes, vars.to.regress = "percent.mt")
```

### Perform linear dimensional reduction

- Next we perform PCA on the scaled data.
- By default, only the previously determined variable features are used as input, but can be defined using `features` argument if you wish to choose a different subset.

```r
dat2Scaled2Obj <- RunPCA(object = dat2ScaledObj, features = VariableFeatures(object = dat2ScaledObj), do.print = TRUE, ndims.print = 1:5, nfeatures.print = 5)
```

- This command uses only the highly variable genes (`VariableFeatures(object = dat2ScaledObj)`) instead of all genes to reduce noise and improve clustering.
- `do.print = TRUE` prints the top genes contributing to each principal component (PC).
- `pcs.print = 1:5` displays the results for PC1 to PC5 (i.e., the first five principal components).
- PC1 (Principal Component 1) is the most important axes of variation.
- PC2, PC3, PC4, etc. explain additional variation in the data.
- Top Positive Genes **(e.g., Ldb2, Ebf1, Prkg1, Meis2)** are more highly expressed in one subset of cells.
- Top Negative Genes **(e.g., Slc27a2, Keg1, Sugct)** are more highly expressed in another subset.
- PC1 may capture a major biological signal, such as cell type differences.
- If PC2 separates a different cell population, it might represent a different biological process, like a cell cycle state or differentiation.

- `DimHeatmap()` allows for easy exploration of the primary sources of heterogeneity in a dataset, and can be useful when trying to decide which PCs to include for further downstream analyses.

```r
DimHeatmap(
    object = dat2Scaled2Obj, 
    dims = 1, 
    balanced = TRUE
)
```

![](images/PC1_dimheatmap.png){width=100%}

- Now, use dims 1 to 15 to plot 15 PCs to observe which PCs contribute to sources of variation in the dataset.

```r
DimHeatmap(
    object = dat2Scaled2Obj, 
    dims = 1:15, 
    balanced = TRUE
)
```

![](images/PC1-15_dimheatmap.png){width=100%}

### Determining dimensionality

- After running PCA using `RunPCA()`, you need to determine how many PCs to retain for clustering and downstream analysis.
- One common method is using the Elbow Plot.
- The top principal components represents highest variations in the dataset.
- So, how many components should we choose to include? 10? 20? 100?
- In ‘Elbow plot’ it ranks principle components based on the percentage of variance explained by each one.

```r
ElbowPlot(dat2Scaled2Obj)
```

![](images/elbowplot.png){width=100%}

- If we are considering going with the default settings with `ndims = 20`, then we can clearly see that at the elbow point is at PC 11 which accounts for most variation in the dataset.

- However, if we choose to include `ndims = 50`, then the elbow plot changes and includes 50 PCs. 

```r
ElbowPlot(dat2Scaled2Obj, ndims = 50)
```

![](images/elbowplot-50.png){width=100%}

- Now, this gives us a much broader picture and also shows more PCs are contributing to variation beyond PC 11.
- With this plot, we can keep the value between PC 21-25.
- In this tutorial, I have chosen PC 25 to stretch the limit and include smaller variance. But the value of 21 will also work fine. 

### Clustering

- Seurat applies a graph-based clustering approach, called Shared Nearest Neighbors (SNN).
- How the SNN Score is Computed:
  - If cell A’s neighbor list and cell B’s neighbor list share many cells, they are likely to be in the same cluster.
  - The SNN score (edge weight) is higher if two cells have more shared neighbors.
- Example:
  - Cell A’s neighbors: {B, C, D, E}
  - Cell B’s neighbors: {A, C, D, F}
  - Cell A and Cell B share: {C, D}
  - SNN score = Number of shared neighbors / Minimum kNN list size
  - SNN score = 2 / 4 = 0.5
- If two cells share many of their nearest neighbors, their SNN score is high, and they are more likely to belong to the same cluster.

![](images/nearest-neighbour.png){width=100%}

- So, the first step is performed using the `FindNeighbors()` function, and takes as input the previously defined dimensionality of the dataset (first 25 PCs).
- To cluster the cells, we next apply modularity optimization techniques such as the Louvain algorithm (default) or SLM to iteratively group cells together.
- The `FindClusters()` function implements this procedure, and contains a resolution parameter that sets the ‘granularity’ of the downstream clustering, with increased values leading to a greater number of clusters.


Run the following commands:

```r
seuratFindNeighbors <- FindNeighbors(
    dat2Scaled2Obj,
    reduction = "pca",
    dims = 1:25,
    do.plot = FALSE,
    graph.name = NULL,
    k.param = 30
)

seuratFindClusters <- FindClusters(object = seuratFindNeighbors)
```

